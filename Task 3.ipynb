{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9328c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataFile = pd.read_csv('./Mixcancer.csv')\n",
    "x = dataFile.values[:, 1:]\n",
    "y = dataFile.values[:, 0]\n",
    "y = y.reshape(len(y),1)\n",
    "x = preprocessing.MinMaxScaler().fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce570290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v):\n",
    "    return 1/(1+np.exp(-v))\n",
    "\n",
    "def sigmoid_der(v):\n",
    "    return sigmoid(v)*(1-sigmoid(v))\n",
    "\n",
    "def reLu(v):\n",
    "    return np.maximum(v,0)\n",
    "\n",
    "def reLu_der(v):\n",
    "    v[v<=0] = 0\n",
    "    v[v>0] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8314a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntrop(o,y):\n",
    "    return (-y*(np.log(o)) - (1-y)* np.log(1-o))\n",
    "\n",
    "def crossEntrDeriv(o,y):\n",
    "    return -(y/o - (1-y)/(1-o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b474b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8421333398124283\n",
      "Loss: 0.16172803319283913\n",
      "Loss: 0.14020356835806155\n",
      "Loss: 0.13153396487041016\n",
      "Loss: 0.1264504848467662\n",
      "Loss: 0.1225861457166549\n",
      "Loss: 0.1189407193142365\n",
      "Loss: 0.11566587865000434\n",
      "Loss: 0.11292275661894993\n",
      "Loss: 0.11009375106043107\n",
      "Loss: 0.1075911639825442\n",
      "Loss: 0.1050815700867344\n",
      "Loss: 0.10301262028734788\n",
      "Loss: 0.10099226516752154\n",
      "Loss: 0.09919750730703725\n",
      "Loss: 0.09726881700079033\n",
      "Loss: 0.09550559103223716\n",
      "Loss: 0.09392159369340596\n",
      "Loss: 0.09241551325051345\n",
      "Loss: 0.09090492027215617\n",
      "Loss: 0.08956765221109862\n",
      "Loss: 0.0881153577626935\n",
      "Loss: 0.0866860040272183\n",
      "Loss: 0.08553401638871773\n",
      "Loss: 0.08443638042678957\n",
      "F-Measure: 0.967741935483871\n",
      "Loss: 0.6754007865291582\n",
      "Loss: 0.14447276979498366\n",
      "Loss: 0.1344755898265812\n",
      "Loss: 0.12806171310076694\n",
      "Loss: 0.1224009309208656\n",
      "Loss: 0.11720529464031205\n",
      "Loss: 0.11253082277903266\n",
      "Loss: 0.10834130373509188\n",
      "Loss: 0.10452819284797862\n",
      "Loss: 0.10112453976730651\n",
      "Loss: 0.0981127884392562\n",
      "Loss: 0.09580982516112037\n",
      "Loss: 0.09358046163159586\n",
      "Loss: 0.0916673315714788\n",
      "Loss: 0.09055002908437329\n",
      "Loss: 0.08947623394508798\n",
      "Loss: 0.08842660618022537\n",
      "Loss: 0.08749831066392774\n",
      "Loss: 0.0868345478018698\n",
      "Loss: 0.08611236027887008\n",
      "Loss: 0.08532274676957054\n",
      "Loss: 0.08473234361331489\n",
      "Loss: 0.084216231721123\n",
      "Loss: 0.08365294233884378\n",
      "Loss: 0.08319569545505102\n",
      "F-Measure: 0.9850746268656716\n",
      "Loss: 0.41843300575521003\n",
      "Loss: 0.14009024029724865\n",
      "Loss: 0.1284178711840082\n",
      "Loss: 0.12542761799711136\n",
      "Loss: 0.1230495628208175\n",
      "Loss: 0.12127524681368888\n",
      "Loss: 0.11762461784393187\n",
      "Loss: 0.11612718243934335\n",
      "Loss: 0.11528594823588222\n",
      "Loss: 0.11433575163921535\n",
      "Loss: 0.1135517129521723\n",
      "Loss: 0.11294719976927885\n",
      "Loss: 0.11265374281370494\n",
      "Loss: 0.11239477999478965\n",
      "Loss: 0.11180615781315471\n",
      "Loss: 0.11205012617451603\n",
      "Loss: 0.11148002972609039\n",
      "Loss: 0.11112688848058777\n",
      "Loss: 0.11093202506025694\n",
      "Loss: 0.11070425621295987\n",
      "Loss: 0.11108467497809217\n",
      "Loss: 0.11077137458398201\n",
      "Loss: 0.11113215009063011\n",
      "Loss: 0.1110671120699713\n",
      "Loss: 0.11076438544537444\n",
      "F-Measure: 0.9538461538461539\n",
      "Loss: 0.5678775418542973\n",
      "Loss: 0.1413317282870213\n",
      "Loss: 0.12697905816868996\n",
      "Loss: 0.11961635915404661\n",
      "Loss: 0.1147019947756991\n",
      "Loss: 0.11085977013992514\n",
      "Loss: 0.10745446665481015\n",
      "Loss: 0.10434739997323099\n",
      "Loss: 0.10145666901575365\n",
      "Loss: 0.09878234986668635\n",
      "Loss: 0.09642667897432174\n",
      "Loss: 0.09444486935784192\n",
      "Loss: 0.0927641050349867\n",
      "Loss: 0.09167877647149586\n",
      "Loss: 0.09108359906552671\n",
      "Loss: 0.09112180671115869\n",
      "Loss: 0.09159531397794814\n",
      "Loss: 0.0924344186851913\n",
      "Loss: 0.09351494704502851\n",
      "Loss: 0.09471865922488898\n",
      "Loss: 0.09580505753781739\n",
      "Loss: 0.09646113833340743\n",
      "Loss: 0.09697818108840568\n",
      "Loss: 0.09757369079891351\n",
      "Loss: 0.0980558502174096\n",
      "F-Measure: 0.9620253164556962\n",
      "Loss: 0.6041528852407819\n",
      "Loss: 0.16322004098445703\n",
      "Loss: 0.14385905130392523\n",
      "Loss: 0.13411574523048939\n",
      "Loss: 0.1275970679733286\n",
      "Loss: 0.12289621844469412\n",
      "Loss: 0.11926197492421538\n",
      "Loss: 0.11662360968732081\n",
      "Loss: 0.11463225255141556\n",
      "Loss: 0.11310769581971332\n",
      "Loss: 0.11203017809150892\n",
      "Loss: 0.11141610855622365\n",
      "Loss: 0.11092699151659553\n",
      "Loss: 0.1105921864035276\n",
      "Loss: 0.11051568279884542\n",
      "Loss: 0.110642649373809\n",
      "Loss: 0.11079031235594135\n",
      "Loss: 0.1106325972276805\n",
      "Loss: 0.11077214496437623\n",
      "Loss: 0.11093013194768427\n",
      "Loss: 0.11098960664884332\n",
      "Loss: 0.11112508801300995\n",
      "Loss: 0.11110380684334391\n",
      "Loss: 0.11096496826626892\n",
      "Loss: 0.11068032723630648\n",
      "F-Measure: 0.9333333333333333\n",
      "Loss: 0.40184483112045455\n",
      "Loss: 0.15425403340613117\n",
      "Loss: 0.14234452183124124\n",
      "Loss: 0.13738898972285807\n",
      "Loss: 0.13457063025809538\n",
      "Loss: 0.1328219409377882\n",
      "Loss: 0.13136776884590892\n",
      "Loss: 0.12852698215078576\n",
      "Loss: 0.12743901931029744\n",
      "Loss: 0.12663980866604538\n",
      "Loss: 0.12596200640584432\n",
      "Loss: 0.12536584458137057\n",
      "Loss: 0.12486202346252306\n",
      "Loss: 0.12438251120098613\n",
      "Loss: 0.12401090163223145\n",
      "Loss: 0.12351484861603268\n",
      "Loss: 0.12323865710726054\n",
      "Loss: 0.12291129572433045\n",
      "Loss: 0.12261707012502651\n",
      "Loss: 0.12231364692966513\n",
      "Loss: 0.12212396150141072\n",
      "Loss: 0.12182615909468819\n",
      "Loss: 0.12154638943450294\n",
      "Loss: 0.12132587922828404\n",
      "Loss: 0.12101967773461474\n",
      "F-Measure: 0.9433962264150945\n",
      "Loss: 0.4047844995046183\n",
      "Loss: 0.15015503202212568\n",
      "Loss: 0.1357869340807563\n",
      "Loss: 0.12597839852778736\n",
      "Loss: 0.11909755576448132\n",
      "Loss: 0.11402832386030927\n",
      "Loss: 0.11031746932453233\n",
      "Loss: 0.10730286184366727\n",
      "Loss: 0.10475290471898346\n",
      "Loss: 0.1024726862419089\n",
      "Loss: 0.10047467602531952\n",
      "Loss: 0.09851528811022978\n",
      "Loss: 0.09699707245183396\n",
      "Loss: 0.09555247497295265\n",
      "Loss: 0.09416129467919848\n",
      "Loss: 0.09273452620745286\n",
      "Loss: 0.09147663540987776\n",
      "Loss: 0.09032149049663893\n",
      "Loss: 0.08891306650286074\n",
      "Loss: 0.08762642962319349\n",
      "Loss: 0.08649735256933463\n",
      "Loss: 0.08541847770068847\n",
      "Loss: 0.08415904215283111\n",
      "Loss: 0.08303103757232856\n",
      "Loss: 0.08199590727153822\n",
      "F-Measure: 0.9846153846153847\n",
      "Loss: 0.32955898410489826\n",
      "Loss: 0.1414661416591158\n",
      "Loss: 0.1255419971604327\n",
      "Loss: 0.11673417040232789\n",
      "Loss: 0.11063721669276978\n",
      "Loss: 0.10595408775152612\n",
      "Loss: 0.10198600206012587\n",
      "Loss: 0.09858757103102396\n",
      "Loss: 0.09560572551877956\n",
      "Loss: 0.09298225881121108\n",
      "Loss: 0.09027151599197793\n",
      "Loss: 0.08800711197513517\n",
      "Loss: 0.0860203178434985\n",
      "Loss: 0.08405240107321446\n",
      "Loss: 0.08226848628997148\n",
      "Loss: 0.08069647923780143\n",
      "Loss: 0.07914247015854631\n",
      "Loss: 0.07760908037102476\n",
      "Loss: 0.07589825517419006\n",
      "Loss: 0.07448309778962506\n",
      "Loss: 0.073080579471402\n",
      "Loss: 0.07202337602549973\n",
      "Loss: 0.07071696085146538\n",
      "Loss: 0.06916575606414714\n",
      "Loss: 0.06789183420759277\n",
      "F-Measure: 0.9846153846153847\n",
      "Loss: 0.508733072587923\n",
      "Loss: 0.1256828389214654\n",
      "Loss: 0.11661577108240449\n",
      "Loss: 0.11058057317897667\n",
      "Loss: 0.10624723804373129\n",
      "Loss: 0.10250129122124733\n",
      "Loss: 0.09933873579945816\n",
      "Loss: 0.09659220507196468\n",
      "Loss: 0.09285895049201943\n",
      "Loss: 0.09071510661086056\n",
      "Loss: 0.0887272544888503\n",
      "Loss: 0.08696728115500893\n",
      "Loss: 0.0853009317857206\n",
      "Loss: 0.0838783411770388\n",
      "Loss: 0.0824629150046995\n",
      "Loss: 0.08109070636533248\n",
      "Loss: 0.07991428908608039\n",
      "Loss: 0.07885213515307794\n",
      "Loss: 0.0777418242895651\n",
      "Loss: 0.07679075009478732\n",
      "Loss: 0.07589036228219184\n",
      "Loss: 0.07498077760267505\n",
      "Loss: 0.07406690155682209\n",
      "Loss: 0.0732049879775606\n",
      "Loss: 0.07237072735099719\n",
      "F-Measure: 0.9787234042553191\n",
      "Loss: 0.5995941906951053\n",
      "Loss: 0.15470336797879308\n",
      "Loss: 0.13496503839233373\n",
      "Loss: 0.12444282622993254\n",
      "Loss: 0.1161555425927385\n",
      "Loss: 0.10954792691502219\n",
      "Loss: 0.1042910295574436\n",
      "Loss: 0.10013200212587302\n",
      "Loss: 0.09666468532770782\n",
      "Loss: 0.09389058574817741\n",
      "Loss: 0.09147053776438234\n",
      "Loss: 0.08941680138534866\n",
      "Loss: 0.08760099441134023\n",
      "Loss: 0.08590486433871794\n",
      "Loss: 0.08462710473855668\n",
      "Loss: 0.08331539087150282\n",
      "Loss: 0.08218584997600219\n",
      "Loss: 0.08050763839602976\n",
      "Loss: 0.07986668940949136\n",
      "Loss: 0.0785859476561649\n",
      "Loss: 0.07726231376800285\n",
      "Loss: 0.07644900605508506\n",
      "Loss: 0.07552477292561702\n",
      "Loss: 0.07442179164359297\n",
      "Loss: 0.07349723665841364\n",
      "F-Measure: 0.9841269841269841\n",
      "Final Accuracy: 0.98\n",
      "Average Accuracy: 0.9560112000000001\n",
      "Average F-Meaure: 0.9677498750012893\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "np.random.seed(0)\n",
    "\n",
    "l = 0.01\n",
    "epochs = 2500\n",
    "batch_size = 8\n",
    "\n",
    "trainError = []\n",
    "testError = []\n",
    "trainAcc = []\n",
    "testAcc = []\n",
    "fMeasure = []\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "    w1 = np.random.uniform(-1,1,[len(x[0]),40]) #weights of the first layer\n",
    "    b1 = np.zeros([1,40]) #bias of the first layer\n",
    "    w2 = np.random.uniform(-1,1,[40,1]) #weights of the second layer\n",
    "    b2 = 0 #bias of the second layer\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, x[train_index].shape[0], batch_size):\n",
    "            x_mini = x[train_index][i:i + batch_size-1, :]\n",
    "            y_mini = y[train_index][i:i + batch_size-1, :]\n",
    "\n",
    "            #feedforward\n",
    "            in1 = x_mini@w1 + b1\n",
    "            o1 = reLu(in1)\n",
    "            in2 = o1@w2 + b2\n",
    "            o2 = sigmoid(in2)\n",
    "\n",
    "            #error calculation\n",
    "            error = crossEntrop(o2 ,y_mini).mean()\n",
    "\n",
    "            #backpropagation Layer 2\n",
    "            dE_dO2 = crossEntrDeriv(o2, y_mini)\n",
    "            dO2_dIn2 = sigmoid_der(in2)\n",
    "            dIn2_dW2 = o1\n",
    "            dIn2_B2 = 1\n",
    "            dE_dW2 = (1/x_mini.shape[0])*dIn2_dW2.T@(dE_dO2*dO2_dIn2)\n",
    "            dE_dB2 = (1/x_mini.shape[0])*np.ones([1,len(x_mini)])@(dE_dO2*dO2_dIn2)\n",
    "\n",
    "            #backpropagation Layer 1\n",
    "            dIn2_dO1 = w2\n",
    "            dO1_dIn1 = reLu_der(in1)\n",
    "            dIn1_dW1 = x_mini\n",
    "            dE_dW1 = (1/x_mini.shape[0])*dIn1_dW1.T@((dE_dO2*dO2_dIn2@dIn2_dO1.T)*dO1_dIn1)\n",
    "            dE_dB1 = (1/x_mini.shape[0])*np.ones([len(x_mini)])@((dE_dO2*dO2_dIn2@dIn2_dO1.T)*dO1_dIn1)\n",
    "\n",
    "            #updating parameters\n",
    "            b2-=l*dE_dB2\n",
    "            w2-=l*dE_dW2\n",
    "            b1-=l*dE_dB1\n",
    "            w1-=l*dE_dW1\n",
    "        \n",
    "        if (epoch % 100 == 0):\n",
    "           print('Loss: ' + str(crossEntrop(sigmoid(reLu(x_test@w1+b1)@w2+b2), y_test).mean()))\n",
    "        \n",
    "        #Accuracy\n",
    "        test_preds = np.where(sigmoid(reLu(x[test_index]@w1+b1)@w2+b2) > 0.5,1,0)\n",
    "        testAcc.append(metrics.accuracy_score(y[test_index], test_preds))\n",
    "        \n",
    "    test_preds = np.where(sigmoid(reLu(x[test_index]@w1+b1)@w2+b2) > 0.5,1,0)\n",
    "    fMeasure.append(metrics.f1_score(test_preds, y[test_index]))\n",
    "    print('F-Measure: ' + str(fMeasure[-1]))\n",
    "    \n",
    "print('Final Accuracy: ' + str(testAcc[-1]))\n",
    "print('Average Accuracy: ' + str(np.mean(testAcc)))\n",
    "print('Average F-Meaure: ' + str(np.mean(fMeasure)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
